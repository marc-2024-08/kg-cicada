{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build A Simple LLM Application with LangChian and Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith\n",
    "Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. THe best way to do this is with LangSmith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ollama](https://ollama.ai/) is one way to easily run inference on macOS.\n",
    "\n",
    "The instructions [here](https://github.com/jmorganca/ollama?tab=readme-ov-file#ollama) provide details, which we summarize:\n",
    "\n",
    "• [Download and run](https://ollama.ai/download) the app  \n",
    "• From command line, fetch a model from this [list of options](https://github.com/jmorganca/ollama): e.g., `ollama pull llama3.1:8b`  \n",
    "• When the app is running, all models are automatically served on `localhost:11434`  \n",
    "\n",
    "Stream tokens as they are being generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...|Neil| Armstrong|!| He| stepped| out| of| the| lunar| module| Eagle| and| onto| the| surface| of| the| moon| on| July| |20|,| |196|9|,| famously| declaring| \"|That|'s| one| small| step| for| man|,| one| giant| leap| for| mankind|.\"||"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.1:latest\")\n",
    "\n",
    "for chunk in llm.stream(\"The first man on the moon was ...\"):\n",
    "    print(chunk, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API Reference: [OllamaLLM](https://python.langchain.com/api_reference/ollama/llms/langchain_ollama.llms.OllamaLLM.html)  \n",
    "Ollama also includes a chat model wrapper that handles formatting conversation turns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n    \"location\": \"Tokyo, Japan\",\\n    \"time_of_day\": \"12:45 PM\"\\n}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "chat_model = ChatOllama(model=\"llama3.1:latest\", format=\"json\")\n",
    "messages = [\n",
    "    (\"human\", \"Return a query for the weather in a random location and time of day with two keys: location and time_of_day. Respond using JSON only.\"),\n",
    "]\n",
    "chat_model.invoke(messages).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API Reference: [ChatOllama](https://python.langchain.com/api_reference/ollama/chat_models/langchain_ollama.chat_models.ChatOllama.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'function': {'name': 'get_current_weather', 'arguments': {'city': 'Toronto'}}}]\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='llama3.1',\n",
    "    messages=[{'role': 'user', 'content': 'What is the weather in Toronto?'}],\n",
    "    \n",
    "    # provide a weather checking tool to the model\n",
    "    tools=[{\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "            'name': 'get_current_weather',\n",
    "            'description': 'Get the current weather for a city',\n",
    "            'parameters': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                    'city': {\n",
    "                        'type': 'string',\n",
    "                        'description': 'The name of the city',\n",
    "                    },\n",
    "                },\n",
    "                'required': ['city'],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response['message']['tool_calls'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
